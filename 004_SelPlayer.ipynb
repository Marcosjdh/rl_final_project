{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from boardgame2 import ReversiEnv\n",
    "import numpy as np\n",
    "from players import RandomPlayer\n",
    "import gym"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SelfPlayer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "En esta notebook se pide armar un entorno al cual se le pase como parámetro la clase de jugador local (DictPlayer, RandomPlayer, GreedyPlayer), y que el entorno devuelva el siguiente paso luego de jugar con el jugador local. Algunas condiciones:\n",
    "- En la función de reset(), se sortearea si el jugador local juega primero o segundo. \n",
    "- El entorno siempre devolverá el tablero como si le tocará jugar al jugador 1. Sea primero o segundo\n",
    "- La clase se instancia con los siguientes parámetros:\n",
    "    - LocalPlayer\n",
    "    - board_shape\n",
    "    \n",
    "- El método step recibirá como parámtro la acción pero codificada no como action = [columna, fila], si no como: action = action[0] * board_shape + action[1]\n",
    "- self.action_space tiene que estar definido acorde al espacio de acción. Por ejemplo: self.action_space = gym.spaces.Discrete(board_shape**2)\n",
    "- self.observation_space también: self.observation_space = gym.spaces.Box(-1, 1, (1, board_shape,board_shape))\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ejemplos"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "El jugador local juega segundo entonces el reset() devuelve (Notar que no se devuelve el player por que siempre juega el 1):"
   ],
   "metadata": {}
  },
  {
   "cell_type": "raw",
   "source": [
    "[[ 0,  0,  0,  0],\n",
    " [ 0,  1, -1,  0],\n",
    " [ 0, -1,  1,  0],\n",
    " [ 0,  0,  0,  0]]"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "El jugador local juega primero entonces el reset() devuelve:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "raw",
   "source": [
    "[[ 0,  0, -1,  0],\n",
    " [ 0, -1, -1,  0],\n",
    " [ 0,  1, -1,  0],\n",
    " [ 0,  0,  0,  0]]"
   ],
   "metadata": {}
  },
  {
   "cell_type": "raw",
   "source": [
    "Que ocurrio aca?\n",
    "\n",
    "El tablero se resetea y queda:\n",
    "\n",
    "[[ 0,  0,  0,  0],\n",
    " [ 0,  1, -1,  0],\n",
    " [ 0, -1,  1,  0],\n",
    " [ 0,  0,  0,  0]]\n",
    " \n",
    "Luego el jugador local muestrea una de las cuatros opciones válidas y juega (0, 2) comiendo la pieza (1, 2) y tranformandola en 1\n",
    "\n",
    "[[ 0,  0,  1,  0],\n",
    " [ 0,  1,  1,  0],\n",
    " [ 0, -1,  1,  0],\n",
    " [ 0,  0,  0,  0]]\n",
    " \n",
    "Ahora le toca el turno al jugador -1 pero el jugador externo tiene que ver el tablero como si fuera el 1, entonces se multiplica el tablero por -1\n",
    "\n",
    "[[ 0,  0, -1,  0],\n",
    " [ 0, -1, -1,  0],\n",
    " [ 0,  1, -1,  0],\n",
    " [ 0,  0,  0,  0]]\n",
    " \n",
    "Ahora el jugador externo seleccionará una acción observando el tablero como si fuera 1.\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "En cuanto a la recompenza tener en cuenta que deberá devolver:\n",
    "- 1 si gana el jugador externo\n",
    "- -1 si gana el LocalPlayer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "board_shape = 8\n",
    "env = ReversiEnv(board_shape=board_shape)\n",
    "(board, player) = env.reset()\n",
    "action = env.action_space # Muestrear acciones uniformemente\n",
    "print(board*-1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1  1  0  0  0]\n",
      " [ 0  0  0  1 -1  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "class SelfPlayEnv(ReversiEnv):\n",
    "    def __init__(self, board_shape=8, LocalPlayer=None):\n",
    "        self.players = [-1, 1]\n",
    "\n",
    "        self.local_player = LocalPlayer(board_shape=board_shape, flatten_action=False)\n",
    "        self.board_shape = board_shape\n",
    "        super(SelfPlayEnv, self).__init__(board_shape=board_shape)\n",
    "        \n",
    "        self.action_space = gym.spaces.Discrete(board_shape**2) # Implementar\n",
    "        self.observation_space = gym.spaces.Box(-1, 1, (1, board_shape,board_shape)) # Implementar\n",
    "         \n",
    "\n",
    "    def play(self, observation):\n",
    "        action = self.local_player.predict(observation)\n",
    "        (observation, self.current_player_num), reward, done, info = super(SelfPlayEnv, self).step(action)\n",
    "\n",
    "        return (observation, self.current_player_num), reward, done, info\n",
    "    \n",
    "    def encode_observation(self, observation, valid_actions=False):\n",
    "        # Implementar\n",
    "        # Simpre devuelve desde el punto de vista del jugador 1\n",
    "        # No devuleve el jugador sino solo el tablero\n",
    "        # Tener en cuenta que esto será la entrada a la red neuronal\n",
    "        board = observation * self.current_player_num\n",
    "        if valid_actions:\n",
    "            return np.array([board, self.get_valid((board, 1))])\n",
    "        else:\n",
    "            return observation * self.current_player_num\n",
    "    \n",
    "    \n",
    "    def reset(self):\n",
    "        self.n_step = 0\n",
    "        self.local_player_num = np.random.choice(self.players)\n",
    "        self.local_player.player = self.local_player_num\n",
    "        self.observation, self.current_player_num = super(SelfPlayEnv, self).reset()\n",
    "        self.allow_pass = True\n",
    "\n",
    "            \n",
    "        if self.current_player_num == self.local_player_num:   \n",
    "            (self.observation, self.current_player_num), _, done, info = self.play(self.observation)\n",
    "            assert done == False\n",
    "\n",
    "        \n",
    "        return self.encode_observation(self.observation)\n",
    "    \n",
    "    def encode_action(self, action):\n",
    "        # Esta es necesario ya que la salida de la red neuronal será un valor entre 0 y board_shape**2 - 1\n",
    "        return [action // self.board_shape, action % self.board_shape]\n",
    "    \n",
    "    def decode_action(self, action):\n",
    "        return action[0] * self.board_shape + action[1]\n",
    "\n",
    "    def step(self, action):\n",
    "        self.n_step += 1\n",
    "        action = self.encode_action(action)\n",
    "        \n",
    "        (self.observation, self.current_player_num), reward, done, _ = super(SelfPlayEnv, self).step(action)   \n",
    "        \n",
    "            \n",
    "        while not done and (self.current_player_num == self.local_player_num):            \n",
    "            (self.observation, self.current_player_num), reward, done, info = self.play(self.observation)\n",
    "\n",
    "        \n",
    "        encoded_observation = self.encode_observation(self.observation)\n",
    "        reward = -float(self.local_player_num*reward) # Implementar. Tiene que ser un float\n",
    "\n",
    "        return encoded_observation, reward, done, {} "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "env = SelfPlayEnv(board_shape=4, LocalPlayer=RandomPlayer)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "env.reset()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0],\n",
       "       [ 0,  1, -1,  0],\n",
       "       [ 0, -1,  1,  0],\n",
       "       [ 0,  0,  0,  0]], dtype=int8)"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Probar entorno"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "def sample_valid_actions(state):\n",
    "    # np.argwhere junto con env.get_valid y randint solucionan el problema en pocas lineas pero puede usar otra estrategia\n",
    "    board_shape = state.shape[0]\n",
    "    # El player es siempre 1\n",
    "    player = 1\n",
    "    valid_actions = np.argwhere(env.get_valid((state, player)) == 1)\n",
    "    action = valid_actions[np.random.randint(len(valid_actions))]\n",
    "    return action[0] * board_shape + action[1]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "done = False\n",
    "board = env.reset()\n",
    "while not done:\n",
    "    action = sample_valid_actions(board)\n",
    "    print(board)\n",
    "    print(f'acion: {action}')\n",
    "    \n",
    "    board, reward, done, _ = env.step(action)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 0  0  0  0]\n",
      " [ 0  1 -1  0]\n",
      " [ 0 -1  1  0]\n",
      " [ 0  0  0  0]]\n",
      "acion: 2\n",
      "[[ 0  0  1  0]\n",
      " [ 0  1  1  0]\n",
      " [ 0 -1 -1 -1]\n",
      " [ 0  0  0  0]]\n",
      "acion: 13\n",
      "[[-1  0  1  0]\n",
      " [ 0 -1  1  0]\n",
      " [ 0  1 -1 -1]\n",
      " [ 0  1  0  0]]\n",
      "acion: 7\n",
      "[[-1  0  1  0]\n",
      " [ 0 -1  1  1]\n",
      " [ 0  1 -1 -1]\n",
      " [ 0  1  0 -1]]\n",
      "acion: 4\n",
      "[[-1  0  1  0]\n",
      " [-1  1  1  1]\n",
      " [-1 -1 -1 -1]\n",
      " [ 0  1  0 -1]]\n",
      "acion: 12\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Entornos vectoriales"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "from multi_env import make_reversi_vec_env"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "source": [
    "board_shape = 8\n",
    "n_envs = 10\n",
    "env = make_reversi_vec_env(\n",
    "    SelfPlayEnv, n_envs=n_envs,\n",
    "    env_kwargs={\n",
    "        'board_shape': board_shape,\n",
    "        'LocalPlayer': RandomPlayer\n",
    "    }\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    "env.reset().shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(10, 1, 8, 8)"
      ]
     },
     "metadata": {},
     "execution_count": 82
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Notar que la entrada tiene como primer componente la cantidad de entornos en paralelo (10), luego la cantidad de canales (1), y finalmente las dimensiones del tablero \n",
    "\n",
    "- Imprimir obs y ver que hay distintas posibles entradas dependiendo de quien juega primero y que jugó el LocalPlayer si le toco primero"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Guardar el SelfPlayEnv en el módulo multi_env para poder despues importarla desde otra notebook"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Instanciamos el modelo con MLP"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "source": [
    "from stable_baselines3 import PPO"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "source": [
    "model = PPO(\n",
    "    'MlpPolicy',\n",
    "    env,\n",
    "    verbose=0,\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "source": [
    "model.policy"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "ActorCriticPolicy(\n",
       "  (features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten()\n",
       "  )\n",
       "  (mlp_extractor): MlpExtractor(\n",
       "    (shared_net): Sequential()\n",
       "    (policy_net): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "    (value_net): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (action_net): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (value_net): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 88
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "source": [
    "obs = env.reset()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "source": [
    "obs.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(10, 1, 8, 8)"
      ]
     },
     "metadata": {},
     "execution_count": 90
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "source": [
    "model.predict(obs)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([19, 18, 18, 23, 17, 40, 18, 47,  0, 18]), None)"
      ]
     },
     "metadata": {},
     "execution_count": 91
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Observaciones:\n",
    "- Lo primero que hace stablebaselines si ponemos MLP es un flatten\n",
    "- Las acciones predichas por el modelo (sin entrentar) tienen una alta probabildad de ser inválidas"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}